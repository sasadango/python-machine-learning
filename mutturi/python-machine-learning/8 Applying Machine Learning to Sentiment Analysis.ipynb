{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Applying Machine Learning to Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本章では、自然言語処理の一分野である感情分析を取り上げる<br>\n",
    "そして、機械学習のアルゴリズムを使用することで、極性に基づいて文書を分類する方法を学ぶ<br>\n",
    "極性とは、書き手の意見のことである<br>\n",
    "本書では、IMDbの50000件の映画レビューで構築されたデータセットを操作し、肯定的または否定的なレビューを分類できる予測機を構築する<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・テキストデータのクレンジングと準備<br>\n",
    "・テキスト文書からの特徴ベクトルの構築  \n",
    "・映画レビューを肯定的な文と否定的な文に分類する機械学習のモデルのトレーニング  \n",
    "・アウトオブコア学習に基づく大規模なテキストデータセットの処理  \n",
    "・文書コレクションからカテゴリのトピックを推定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1 IMDbの映画データセットでのテキスト処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感情分析は、広大な分野であるNLPの一分野としてよく知られており、文書の極性を分析することに関連している  \n",
    "感情分析は、意見マイニングとも呼ばれる  \n",
    "感情分析でよく知られているタスクは、ある話題に関して書き手が表明した意見や感情に基づいて文書を分類することである  \n",
    "映画レビューのサブセットから意味のある情報を抽出し、レビューした人が映画を「好き」と評価したのか、「嫌い」と評価したのかを予測できる機械学習モデルの構築方法について説明する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1.1 映画レビューデータセットを取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonを使ってtarfileアーカイブを直接展開する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "with tarfile.open('aclImdb_v1.tar.gz', 'r:gz') as tar:\n",
    "    tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.1.2 映画レビューデータセットをより便利なフォーマットに変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットを取り出したら、ダウンロードアーカイブに含まれていたテキスト文書を1つのCSVファイルにまとめる  \n",
    "次のコードは、映画レビューをpandasのDataFrameオブジェクトに読み込む  \n",
    "この処理には、標準的なデスクトップコンピュータで10分ほどかかることがある  \n",
    "進行状況と推定時間を確認するにはPyPrindパッケージを使用する  \n",
    "PyPrindをインストールするには、pip install pyprindコマンドを実行する  \n",
    "インストールが完了したら、次のコードを実行できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:04:01\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "# 'bashpath'の値を展開した映画レビューデータセットのディレクトリに置き換える\n",
    "basepath = 'aclImdb'\n",
    "labels = {'pos':1, 'neg':0}\n",
    "pbar = pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "            \n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードでは、まずプログレスオーバーオブジェクトpbarを50000回のいてレーションで初期化している  \n",
    "この回数は読み込みの対象となる文書の個数に匹敵する  \n",
    "入れ子のforループを使ってaclImdbディレクトリのtrainサブディレクトリとtestサブディレクトリを処理し、posサブディレクトリとnegサブディレクトリから個々のテキストファイルを読み込んでいる  \n",
    "そして最後に、dfというDataFrameオブジェクトにそれらのファイルを整数のクラスラベルとともに追加している  \n",
    "1のクラスラベルは「肯定的」、0のクラスラベルは「否定的」を表す  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセットに組み込まれているクラスラベルはソート済みであるため、次のコードに示すように、np.randomサブモジュールのpermutation関数を使って行の順番をシャッフルしたDataFrameオブジェクトを作成する  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータセットは後程使用することになるため、データを正しいフォーマットで保存出来ていることを簡単に確認しておく  \n",
    "CSVファイルを読み込み、最初の3つのサンプルから抜粋したデータを出力する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2 BoWモデルの紹介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文章や単語などのカテゴリーは、機械学習アルゴリズムに渡す前に数値に変換しておく必要がある  \n",
    "ここでは、テキストを数値の特徴ベクトルとして表現できるBoWモデルを紹介する  \n",
    "BoWもでるの背景にある考え方はとても単純で、次のように要約できる  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.文書の集合全体から、たとえば単語という一意なトークンからなる語彙を作成する  \n",
    "2.各文書での各単語の出現回数を含んだ特徴ベクトルを構築する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各文書において一意な単語は、BoWの語彙を構成しているすべての単語の一部に過ぎない  \n",
    "このとき、特徴ベクトルの大半は0になるため、疎ベクトルと呼ばれる  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2.1 単語を特徴ベクトルに変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各文書に含まれる単語に基づいてBoWモデルを構築するには、scikit-learnに実装されているConutVectorizerクラスを使用できる  \n",
    "次のコードに示すように、このクラスはテキストデータの配列を入力として、BoWモデルを自動的に生成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining, the weather is sweet, and one and one is two'])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizerクラスのfit_transformメソッドを呼び出すことで、BoWモデルの語彙を生成し、次の3つの文章を疎なベクトルに変換している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'The sun is shining'\n",
    "- 'The weather is sweet'\n",
    "- 'The sun is shining, the weather is sweet, and one and one is two'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコマンドを実行するとわかるように、語彙はディクショナリに格納されている  \n",
    "このディクショナリの要素は一意な単語と整数値を対応づけたものであり、たとえば'and'と0を対応付ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この特徴ベクトルの各要素のインデックスは、ディクショナリの整数値に対応している  \n",
    "例えば、整数値0に対応する最初の特徴量は、最後の文書にのみ出現する単語'and'の個数である  \n",
    "整数値1に対応する単語'is'は、文書ベクトルの2つ目の特徴量であり、3つの文章のすべてに出現している  \n",
    "特徴ベクトルにおけるそれらの値は生の出現頻度ともよばれ、tf(t,d)で表される  \n",
    "これは文書dにおける単語tの出現回数を表す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2.2 TF-IDFを使って単語の関連性を評価する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テキストデータを解析していると、「肯定的」、「否定的」など両極のクラスそれぞれに分類される複数の文書において、同じ単語が出現することがよくある  \n",
    "そうした頻繁に出現する単語は、たいてい、意味のある情報や判別情報を含んでいない  \n",
    "ここでは、TF-IDFという便利な手法について説明する  \n",
    "この手法を利用すれば、特徴ベクトルに頻繁に出現する単語の重みを減らすことができる  \n",
    "TF-IDFは、TF（単語の出現頻度）とIDF（逆文書頻度）の積として定義できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf(t,d) = tf(t,d)×idf(t,d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで、tf(t,d)は、前項で説明した単語の出現頻度である  \n",
    "逆文書頻度idf(t,d)は次の方法で求めることができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf(t,d) = log\\frac{n_d}{1+df(t,d)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_d$は文書の総数、df(t,d)は単語tを含んでいる文書dの個数を表す  \n",
    "母数に定数1を足すのは、トレーニングサンプルに出現するすべての単語に0以外の値を割り当てることで、ゼロ割を回避するためである  \n",
    "また、対数が使用されているのは、頻度の低い文書に過剰な重みが与えられないようにするためである"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scilit-learnには、TfidfTransformerクラスという変換器も実装されている  \n",
    "このクラスは、CountVectorizer（のfit_transformメソッド）から「生の単語の出現頻度」を入力として受け取り、それらをTF-IDFに変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.43  0.    0.56  0.56  0.    0.43  0.    0.  ]\n",
      " [ 0.    0.43  0.    0.    0.    0.56  0.43  0.    0.56]\n",
      " [ 0.5   0.45  0.5   0.19  0.19  0.19  0.3   0.25  0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語'is'の出現頻度が最も高いのは3つ目の文書であり、再頻出単語となっている  \n",
    "だが、単語'is'は文書１と文書２にも含まれている．したがって、有益な判別情報を含んでいるとは考えにくい  \n",
    "そこで、前項の特徴ベクトルをTF-IDFに変換すれば、文書３において単語'is'がそれほど大きくないTF-IDF（0.45）に関連付けられることが確認できるはずだ  \n",
    "対照的に、単語'one'は3つ目の文章で2回出現しており、TF-IDFはより大きい0.5である  \n",
    "単語'one'は3つ目の文書だけに出現しており、より判別的な情報になっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特徴ベクトルの単語のTF-IDFを手動で計算すれば、先に定義した教科書どおりの「標準的」な定義式と比べて、TfidfTransformerによるTF-IDFの計算が少し異なることがわかる  \n",
    "まず、scikit-learnに実装されているIDFとTF-IDFの式は次の通り"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf(t,d) = log\\frac{1+n_d}{1+df(t,d)}+1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同様に、scikit-learnで計算されるTD-IDFは、先に定義したデフォルトの式とは少し異なっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf(t,d) = tf(t,d)×idf(t,d)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFを計算する前に「生の単語の出現頻度」を正規化するのがより一般的である  \n",
    "ただし、TfidTransformerはTF-IDFを直接正規化するため、この方法を検討してみる  \n",
    "デフォルト（norm='l2'）では、scikit-learnのTfidfTransformerはL2正規化を適用する  \n",
    "その場合は、正規化されていない特徴ベクトルvをL2正規化で割ることにより、長さ１のベクトルが返される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{(\\sum_{i=1}^{n}{v_{i}^{2}})^{1/2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfTransformerの仕組みを理解していることを確認するため、例を追いかけながら、3つ目の文書に対して単語'is'のTF-IDFを計算してみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単語'is'の3つ目の文書での出現頻度は3（TF=3）である  \n",
    "この単語は3つの文書のすべてに出現するため、この単語の文書頻度は3（DF=3）である  \n",
    "よって、IDFを次のように計算できる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$idf(\"is\",d_3) = log\\frac{1+3}{1+3} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDFを計算するには、IDFに1を足し、それにTFを掛ければよい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf(\"is\",d_3) = 3×(0+1) = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この計算を3つ目の文書のすべての単語で繰り返すと、次のTF-IDFベクトル[3.39, 3.0, 3.39, 1.29, 1.29, 1.29, 2.0, 1.69, 1.29]が得られる  \n",
    "ただし、この特徴ベクトルの値が先ほど使用したTfidfTransformerから取得した値とは異なることが分かる  \n",
    "このTF-IDFの計算に足りない最後の手順は、L2正規化である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tf-idf(\"is\",d_3)_(norm) = 0.45$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、scikit-learnのTfidfTransformerから返された結果と一致することが分かる  \n",
    "TF-IDFがどのように計算されるのかを理解したところで、次項では、このような考え方を映画レビューデータセットに適用してみることにしよう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2.3 テキストデータのクレンジング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの項では、BoWモデル、単語の出現頻度、TF-IDFについて説明した  \n",
    "だが不要な文字をすべて取り除くことにより、テキストデータをクレンジングすることが最初の重要な手順となる  \n",
    "これがなぜ重要なのかを説明するために、再びシャッフルした映画レビューデータセットの1つ目の文書から、最後の50文字を出力してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0, 'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "見てのとおり、このテキストにはHTMLマークアップに加えて、句読点やその他の非英字文字列が含まれている  \n",
    "HTMLマークアップはそれほど重要な意味を含んでいないものの、文脈によっては、句読点は有益な追加情報を表すことがある  \n",
    "ただし、ここでは話を単純にするために、感情分析に役立つ\":)\"のような顔文字だけを残し、それ以外の句読点はすべて削除する  \n",
    "そのためpreprocessor関数を定義し、そこでPythonの正規表現ライブラリであるreを使用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re \n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ''.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このコードでは、1つ目の正規表現<[^>] * >を使用することで、映画レビューに含まれているHTMLマークアップを完全に削除しようとしている  \n",
    "多くのプログラマは概してHTMLの解析に正規表現を使用しないように勧めてはいるが、このデータセットなら、この正規表現で十分に「クレンジング」できるはずだ  \n",
    "HTMLマークアップを削除した後、もう少し複雑な正規表現を使って顔文字を検索し、一時的にemoticonsとして格納している  \n",
    "次に、正規表現[\\W]+を使って単語の一部ではない文字を全て削除し、テキストを小文字に変換している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて、一時的に格納したemoticonsを処理済みの文書文字列の末尾に付け足している  \n",
    "さらに、一貫性を保つために、顔文字から「鼻」文字（-）を削除している"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クレンジングした文書文字列の末尾に顔文字を追加するのは、特に洗練された方法には思えないかもしれない  \n",
    "だが、このBoWモデルでは、語彙を構成しているトークンが1つの単語に過ぎないしたら、単語の順序は重要ではない  \n",
    "文書を個々の言葉や単語、トークンに分割する方法についてさらに説明する前に、先に定義したpreprocessor関数が正しく動作することを確認しておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven title brazil not available'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(df.loc[0, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :):(:)'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor(\"</a>This :) is :( a test :-)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、以降の説明では「クレンジングした」テキストデータを繰り返し使用することになる  \n",
    "そのため、DataFrameオブジェクトに含まれているすべての映画レビューにpreprpcessor関数を適用しておこう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.2.4 文書をトークン化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
